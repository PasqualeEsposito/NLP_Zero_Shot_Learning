{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJPj9uo4aNFW"
      },
      "source": [
        "## SETUP AND DATASET LOADING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeYCgPuV72aG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4cCxoS1HvT2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoMYJDpcHvRM"
      },
      "outputs": [],
      "source": [
        "path_prefix = \"/content/gdrive/MyDrive/DATASETS/NLI/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-Y6KJ8PHvOg"
      },
      "outputs": [],
      "source": [
        "cd /content/gdrive/MyDrive/DATASETS/NLI/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWUtm4a_HvMF"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt2Uo1NSHvHz"
      },
      "outputs": [],
      "source": [
        "NLI_DATASETS = [\"scitail\", \"anli\", \"glue-mnli\", \"sick\", \"superglue-cb\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KScehrvMSfVH"
      },
      "source": [
        "superglue cb\n",
        "sick\n",
        "scitail\n",
        "anli\n",
        "glue-mnli\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rjOy2xTJTyW"
      },
      "outputs": [],
      "source": [
        "NLI_DATASETS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Scz4sjfPHvFv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UEnQnb0JnU0"
      },
      "outputs": [],
      "source": [
        "DATASET_SEEDS = [13, 21, 42, 87, 100]\n",
        "WORDS_TO_REMOVE = [\"context:\", \"premise:\", \"sentence 1:\", \"sentence 2:\", \"sentence:\", \"hypothesis:\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVsYfCFNHvAn"
      },
      "outputs": [],
      "source": [
        "def import_dataset(dataset_index, test_set = False):\n",
        "  dataset_name = NLI_DATASETS[dataset_index]\n",
        "  print(dataset_name)\n",
        "  seed = 100 #test_set is equal independently on random seed\n",
        "  if test_set == False:\n",
        "    test_set_path = f\"{dataset_name}/{dataset_name}_16_{seed}_dev.tsv\"\n",
        "  else:\n",
        "    test_set_path = f\"{dataset_name}/{dataset_name}_16_{seed}_test.tsv\"\n",
        "  #print(test_set_path)\n",
        "  df = pd.read_csv(test_set_path , sep=\"\\t\", header=None)\n",
        "\n",
        "  df[0] = df[0].apply(preprocess_nli)\n",
        "  #print(df[0][0])\n",
        "  return (df[0], df[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WmI81aWYH-v"
      },
      "outputs": [],
      "source": [
        "def preprocess_nli(query):\n",
        "  for substr in  WORDS_TO_REMOVE:\n",
        "    query = query.replace(substr, \"\")\n",
        "\n",
        "  ph = re.split(\"\\[SEP\\]\", query)\n",
        "  #return f\"premise:{ph[0]} hypothesis:{ph[1]}\"\n",
        "  return (ph[0], ph[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpFrqbmAHu6k"
      },
      "outputs": [],
      "source": [
        "for dataset_inex in range (0, len(NLI_DATASETS)):\n",
        "  import_dataset(dataset_inex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYV9tmz4u3if"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"anli/an_16_100_test.tsv\", sep=\"\\t\", header = None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "VgaT7LILYW4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDKP-1uGIi4P"
      },
      "source": [
        "## PROMPTS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoTg3kbOuuL3"
      },
      "source": [
        "The scitail dataset is setted up for binary NLI: possible answeres are entailment and not entailment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-oQ_ir2ut82"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoHi7D5Vhixg"
      },
      "outputs": [],
      "source": [
        "NLI_TRUE_FALSE_PROMPTS = [\n",
        "  \"Premise: {CONTEXT}\\n\\nHypothesis: {HYPOTHESIS}\\n\\nDoes the premise entails the hypothesis?\\n\\n{OPTIONS}\",\n",
        "  \"Premise: {CONTEXT}\\nHypothesis: {HYPOTHESIS}\\nIs the hypothesis entailed by the premise?\\n{OPTIONS}\",\n",
        "  \"Here is a premise:\\n{CONTEXT}\\n\\nHere is a hypothesis:\\n{HYPOTHESIS}\\n\\ nIs it possible to conclude that if the premise is true, then so is the hypothesis?\\n{OPTIONS}\",\n",
        "  \"Sentence 1: {CONTEXT}\\n\\nSentence 2: {HYPOTHESIS}\\nIs this second sentence entailed by the first sentence?\\n\\n{OPTIONS}\",\n",
        "  \"Sentence 1: {CONTEXT}\\n\\nSentence 2: {HYPOTHESIS}\\n\\nIf the first sentence is true, then is the second sentence true?\\n{OPTIONS}\",\n",
        "  'Based on the premise \\\"{CONTEXT}\\\", can we conclude the hypothesis \\\"{HYPOTHESIS}\\\" is true?\\n\\n{OPTIONS}',\n",
        "  'Premise: \\\"{CONTEXT}\\\" If this premise is true, what does that tell us about whether it entails the hypothesis \\\"{HYPOTHESIS}\\\"?\\n\\n{OPTIONS}',\n",
        "  'Premise:\\n\\\"{CONTEXT}\\\" Based on this premise, is the hypothesis \\\"{HYPOTHESIS}\\\" true?\\n {OPTIONS}',\n",
        "  'If {CONTEXT}, can we conclude that \\\"{HYPOTHESIS}\\\"?\\n{OPTIONS}',\n",
        "  '{CONTEXT}\\n\\nDoes it follow that \\\"{HYPOTHESIS}\\\"?\\n{OPTIONS}'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOkwMWUTIoWi"
      },
      "outputs": [],
      "source": [
        "NLI_PROMPTS_SINGLE_SENTENCE = [\n",
        "    \"Choose the correct label among: {OPTIONS} for the following Natural Language Inference task: {SENTENCE}\",\n",
        "    \"Read the follwing sentence. sentence: {SENTENCE}. Is it an 'entailment', a 'contradiction' or is it 'neutral'?\",\n",
        "    \"Assign one of the following labels: {OPTIONS} to the following sentence S: {SENTENCE}\" ,\n",
        "    \"The following assertion: '{SENTENCE}' is {OPTIONS}?\",\n",
        "    \"A sentence can be of one type among: {OPTIONS}. Which type of sentence is [{SENTENCE}]?\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mvBECr3qdCf"
      },
      "outputs": [],
      "source": [
        "NLI_PROMPT_CONTEXT_HYPO = [\n",
        "    \"{CONTEXT}\\nBased on the paragraph above can we conclude that the hypothesis: \\\"{HYPOTHESIS}\\\" is more likely to be a kind of {OPTIONS}? Choose only one of answer among theese three.\",\n",
        "    \"{CONTEXT}\\n\\nBased on that paragraph can we conclude that this sentence is true?\\n{HYPOTHESIS}\\n\\n{OPTIONS}\",\n",
        "    \"{CONTEXT}\\n\\nCan we draw the following conclusion?\\n{HYPOTHESIS}\\n\\n{OPTIONS}\"\n",
        "    \"{CONTEXT}\\nDoes this next sentence follow, given the preceding text?\\n{HYPOTHESIS}\\n\\n{OPTIONS}\",\n",
        "    \"{CONTEXT}\\nCan we infer the following?\\n{HYPOTHESIS}\\n\\n{OPTIONS}\",\n",
        "    \"Read the following paragraph and determine if the hypothesis is true:\\n\\n{CONTEXT}\\n\\nHypothesis: {HYPOTHESIS}\\n\\n{OPTIONS}\",\n",
        "    \"Read the text and determine if the sentence is true:\\n\\n{CONTEXT}\\n\\nSentence: {HYPOTHESIS}\\n\\n{OPTIONS}\",\n",
        "    \"Can we draw the following hypothesis from the CONTEXT? \\n\\nCONTEXT:\\n\\n{CONTEXT}\\n\\nHypothesis: {HYPOTHESIS}\\n\\n{OPTIONS}\",\n",
        "    \"Determine if the sentence is true based on the text below:\\n{HYPOTHESIS}\\n\\n{CONTEXT}\\n{OPTIONS}\",\n",
        "    \"Premise: {CONTEXT}\\n\\nHypothesis: {HYPOTHESIS}\\n\\nDoes the premise entail the hypothesis?\\n\\n{OPTIONS}\",\n",
        "    \"Premise: {CONTEXT}\\nHypothesis: {HYPOTHESIS}\\nIs the hypothesis entailed by the premise?\\n{OPTIONS}\",\n",
        "    \"Here is a premise:\\n{CONTEXT}\\n\\nHere is a hypothesis:\\n{HYPOTHESIS}\\n\\nIs it possible to conclude that if the premise is true, then so is the hypothesis?\\n{OPTIONS}\",\n",
        "    \"Sentence 1: {CONTEXT}\\n\\nSentence 2: {HYPOTHESIS}\\nIs this second sentence entailed by the first sentence?\\n\\n{OPTIONS}\",\n",
        "    \"Sentence 1: {CONTEXT}\\n\\nSentence 2: {HYPOTHESIS}\\n\\nIf the first sentence is true, then is the second sentence true?\\n{OPTIONS}\",\n",
        "    'Based on the premise \"{CONTEXT}\", can we conclude the hypothesis {HYPOTHESIS}\" is true?\\n\\n{OPTIONS}',\n",
        "    'Premise: \"{CONTEXT}\" If this premise is true, what does that tell us about whether it entails the hypothesis \"{HYPOTHESIS}\"?\\n\\n{OPTIONS}',\n",
        "    'Premise:\\n\"{CONTEXT}\" Based on this premise, is the hypothesis \"{HYPOTHESIS}\" true?\\n{OPTIONS}',\n",
        "    'If {CONTEXT}, can we conclude that \"{HYPOTHESIS}\"?\\n{OPTIONS}',\n",
        "    '{CONTEXT}\\n\\nDoes it follow that \"{HYPOTHESIS}\"?\\n{OPTIONS}'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p10HB_Sxk5c6"
      },
      "outputs": [],
      "source": [
        "NLI_PROMPT_CONTEXT_HYPO[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ujbu4Z2tLPBZ"
      },
      "outputs": [],
      "source": [
        "from random import shuffle\n",
        "\n",
        "def adapt_prompt(prompt_index, sentence, context_hypo_type = False, two_options_nli = False):\n",
        "  if two_options_nli == False:\n",
        "    options = ['entailment','neutral', 'contradiction']\n",
        "    shuffle(options)\n",
        "    stringyfied_options = f\"['{options[0]}', '{options[1]}', '{options[2]}']\";\n",
        "\n",
        "    if context_hypo_type:\n",
        "      context, hypo = sentence;\n",
        "      return NLI_PROMPT_CONTEXT_HYPO[prompt_index].format(OPTIONS=stringyfied_options, CONTEXT=context, HYPOTHESIS=hypo)\n",
        "    else:\n",
        "      return NLI_PROMPTS_SINGLE_SENTENCE[prompt_index].format(OPTIONS = options, SENTENCE = sentence)\n",
        "  #if !two_options_nli:\n",
        "  else:\n",
        "    options = ['true','false']\n",
        "    context, hypo = sentence;\n",
        "    return NLI_TRUE_FALSE_PROMPTS[prompt_index].format(OPTIONS=options, CONTEXT=context, HYPOTHESIS=hypo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taSnFAUBZpO7"
      },
      "source": [
        "##MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGX7osVXvgFf"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHotcejb_NRx"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkJiNZsNS9KV"
      },
      "source": [
        "### FLAN T5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxLZ5aMRDLne"
      },
      "source": [
        "While we qualitatively find that FLAN responds well to most tasks, it does fail on some simple tasks.\n",
        "For instance, as shown in Figure 22, FLAN fails at the very simple task of returning the second word\n",
        "in a sentence, and also incorrectly translates a question to Danish when asked to answer the question\n",
        "in Danish. Additional limitations include a context length of only 1024 tokens (which is not enough\n",
        "for most summarization tasks), and that the model was mostly trained on English data.\n",
        "\n",
        "https://openreview.net/pdf?id=gEZrGCozdqR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RPk1z6yvjct"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y69muJbD-gLZ"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "flan_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "flan_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgG63c70hCrb"
      },
      "outputs": [],
      "source": [
        "def askFlanNLI(data, solutions, prompt_index = 0, context_hypo_prompt = False, two_options_nli=False):\n",
        "  results = []\n",
        "  for sentence in data:\n",
        "    task_prefix = adapt_prompt(prompt_index, sentence, context_hypo_prompt, two_options_nli)\n",
        "\n",
        "    comp = solutions.tolist()\n",
        "\n",
        "    input_ids = flan_tokenizer(task_prefix, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    output_sequences = flan_model.generate(input_ids=input_ids[\"input_ids\"],\n",
        "                                      max_length=1000)\n",
        "    result = flan_tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "    result = result[0].lower()\n",
        "\n",
        "    if not two_options_nli and (result == 'no' or result ==  'conflict'):\n",
        "      results.append('contradiction')\n",
        "    elif result == 'yes':\n",
        "      results.append('entailment')\n",
        "    else:\n",
        "      results.append('neutral')\n",
        "\n",
        "  for pos in range (0, len(results)):\n",
        "    #print(results[pos])\n",
        "    if(results[pos] == comp[pos]):\n",
        "      results[pos] = True\n",
        "    else:\n",
        "      results[pos] = False\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdHS26IXJaBI"
      },
      "source": [
        "### BART\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Di7PzbOz_AzK"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "bart_classifier = pipeline(\"zero-shot-classification\",\n",
        "                      model=\"facebook/bart-large-mnli\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCZOAzsz2F9j"
      },
      "outputs": [],
      "source": [
        "def askBartNLI(data, solutions, prompt_index=None, context_hypo_prompt = False, two_options_nli=False):\n",
        "  results = []\n",
        "  solutions = solutions.tolist()\n",
        "\n",
        "  #Also check the non classifier version and the unprompetd version for the classifier\n",
        "\n",
        "  for i in range (, len(data)):\n",
        "    if prompt_index != None:\n",
        "      prompt = adapt_prompt(prompt_index, data[i], context_hypo_prompt, two_options_nli)\n",
        "    else:\n",
        "      prompt = data[i]\n",
        "\n",
        "    #print(classifier(data[i], ['contradiction', 'neutral', 'entailment']))\n",
        "    if two_options_nli:\n",
        "      answer = bart_classifier(prompt, ['true', 'false'])['labels'][0]\n",
        "\n",
        "      if answer == 'false':\n",
        "        answer = 'neutral'\n",
        "      else:\n",
        "        answer = 'entailment'\n",
        "\n",
        "    else:\n",
        "      answer = bart_classifier(prompt, ['contradiction', 'neutral', 'entailment'])['labels'][0]\n",
        "\n",
        "    #print(answer)\n",
        "    if(answer == solutions[i]):\n",
        "      results.append(True)\n",
        "    else:\n",
        "      results.append(False)\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuxxN1OOUWb_"
      },
      "source": [
        "### GPT2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4F3GWCiokHc"
      },
      "outputs": [],
      "source": [
        "# Check if larger versions exist and can be run.\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIbK7KaTmgGi"
      },
      "outputs": [],
      "source": [
        "from string import Template\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "#import streamlit as st\n",
        "\n",
        "def gpt2_classify(inputs, candidate_labels, prompt=\"$input is a type of \"):\n",
        "\n",
        "    if type(candidate_labels) == list:\n",
        "        candidate_labels = {k: [k] for k in candidate_labels}\n",
        "\n",
        "    # Add the final space of the prompt to each candidate label\n",
        "    if prompt[-1] == \" \":\n",
        "        content_prefix, prompt = \" \", prompt[:-1]\n",
        "    else:\n",
        "        content_prefix = \"\"\n",
        "\n",
        "    # Encode the part of the prompt which is independent of the inputs\n",
        "    prompt_prefix = prompt.split(\"$input\")[0]\n",
        "    if len(prompt_prefix) > 0:\n",
        "        prompt = prompt[len(prompt_prefix) :]\n",
        "        if prompt_prefix[-1] == \" \":\n",
        "            prompt_prefix, prompt = prompt_prefix[:-1], \" \" + prompt\n",
        "        inputs_prefix = gpt2_tokenizer(prompt_prefix, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            past_key_values_prefix = gpt2_model(**inputs_prefix).past_key_values\n",
        "        del inputs_prefix\n",
        "\n",
        "    scores = []\n",
        "    # For each input, encode the input-dependent part of the prompt\n",
        "    for input in inputs:\n",
        "        inputs = gpt2_tokenizer.encode(Template(prompt).substitute(input=input))\n",
        "        inputs = torch.tensor([inputs])\n",
        "        with torch.no_grad():\n",
        "            if len(prompt_prefix) > 0:\n",
        "                outputs = gpt2_model(inputs, past_key_values=past_key_values_prefix)\n",
        "            else:\n",
        "                outputs = gpt2_model(inputs)\n",
        "        del inputs\n",
        "        probs = torch.softmax(outputs.logits[0, -1, :], -1).detach().cpu().numpy()\n",
        "        past_key_values = outputs.past_key_values\n",
        "        del outputs\n",
        "\n",
        "        scores.append({\"sequence\": input})\n",
        "        scores[-1][\"labels\"] = [k for k in candidate_labels]\n",
        "        scores[-1][\"scores\"] = []\n",
        "        # Get the probability of each candidate label being the next tokens\n",
        "        for k in candidate_labels:\n",
        "            scores[-1][\"scores\"].append(0)\n",
        "            for word in candidate_labels[k]:\n",
        "                tokens = gpt2_tokenizer.encode(content_prefix + word)\n",
        "                prob = probs[tokens[0]]\n",
        "                if len(tokens) > 1:\n",
        "                    with torch.no_grad():\n",
        "                        outputs2 = gpt2_model(\n",
        "                            torch.tensor([tokens[:-1]]),\n",
        "                            past_key_values=past_key_values,\n",
        "                        )\n",
        "                    probs2 = (\n",
        "                        torch.softmax(outputs2.logits[0, :, :], -1)\n",
        "                        .detach()\n",
        "                        .cpu()\n",
        "                        .numpy()\n",
        "                    )\n",
        "                    del outputs2\n",
        "                    for i in range(1, len(tokens)):\n",
        "                        prob *= probs2[i - 1, tokens[i]]\n",
        "                scores[-1][\"scores\"][-1] += prob\n",
        "        # Normalize the scores to get probabilities\n",
        "        sum_scores = sum(scores[-1][\"scores\"])\n",
        "        scores[-1][\"scores\"] = [x / sum_scores for x in scores[-1][\"scores\"]]\n",
        "\n",
        "    return scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6V557w0xOHc"
      },
      "outputs": [],
      "source": [
        "def askGPT2NLI(premise, solutions, prompt_index=None,  context_hypo_prompt = False, two_options_nli=False):\n",
        "\n",
        "  res = []\n",
        "  results = []\n",
        "  solutions = solutions.tolist()\n",
        "  for i in range (0, len(premise)):\n",
        "\n",
        "    if prompt_index != None :\n",
        "      prompt = adapt_prompt(prompt_index, premise[i],  context_hypo_prompt, two_options_nli)\n",
        "    else:\n",
        "      prompt = premise[i]\n",
        "\n",
        "    if two_options_nli:\n",
        "      res = gpt2_classify([prompt], ['true', 'false'])\n",
        "      answer = res[0][\"labels\"][res[0][\"scores\"].index(max(res[0][\"scores\"]))]\n",
        "      if answer == 'false':\n",
        "        answer = 'neutral'\n",
        "      else:\n",
        "        answer = 'entailment'\n",
        "\n",
        "    else:\n",
        "      res = gpt2_classify([prompt], ['contradiction', 'neutral', 'entailment'])\n",
        "      answer = res[0][\"labels\"][res[0][\"scores\"].index(max(res[0][\"scores\"]))]\n",
        "\n",
        "    #print(res[0][\"labels\"][res[0][\"scores\"].index(max(res[0][\"scores\"]))])\n",
        "    if(answer == solutions[i]):\n",
        "      results.append(True)\n",
        "    else:\n",
        "      results.append(False)\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2onzu89NUY6W"
      },
      "source": [
        "### GPT3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Sd8Rukqsy9e"
      },
      "outputs": [],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AozewMcCs40g"
      },
      "outputs": [],
      "source": [
        "import openai as ai\n",
        "\n",
        "ai.api_key = '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' # replace with your key from earlier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO0Dj9QFul4B"
      },
      "outputs": [],
      "source": [
        "def generate_gpt3_response(user_text, print_output=False):\n",
        "    \"\"\"\n",
        "    Query OpenAI GPT-3 for the specific key and get back a response\n",
        "    :type user_text: str the user's text to query for\n",
        "    :type print_output: boolean whether or not to print the raw output JSON\n",
        "    \"\"\"\n",
        "    completions = ai.Completion.create(\n",
        "        engine='text-davinci-003',  # Determines the quality, speed, and cost.\n",
        "        temperature=0.5,            # Level of creativity in the response\n",
        "        prompt=user_text,           # What the user typed in\n",
        "        max_tokens=100,             # Maximum tokens in the prompt AND response\n",
        "        n=1,                        # The number of completions to generate\n",
        "        stop=None,                  # An optional setting to control response generation\n",
        "    )\n",
        "\n",
        "    # Displaying the output can be helpful if things go wrong\n",
        "    if print_output:\n",
        "        print(completions)\n",
        "\n",
        "    # Return the first choice's text\n",
        "    return completions.choices[0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj23kKpSpizl"
      },
      "outputs": [],
      "source": [
        "def askGPT3NLI(query, answers, prompt_index=0, context_hypo_prompt = False, two_options_nli=False):\n",
        "  results = []\n",
        "  model_answers = []\n",
        "\n",
        "  for elem in query:\n",
        "      prompt = adapt_prompt(prompt_index, query, context_hypo_prompt,two_options_nli)\n",
        "      results.append(generate_gpt3_response(prompt)) ## TODO check if answer is correct in a second moment.\n",
        "\n",
        "  for pos in range (0, len(results)):\n",
        "    print(results[pos])\n",
        "    if(results[pos] == answers[pos]):\n",
        "      results[pos] = True\n",
        "    else:\n",
        "      results[pos] = False\n",
        "\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkH9GoTiS7s8"
      },
      "source": [
        "##  EXPERIMENTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8NebOc6DKg9"
      },
      "source": [
        "### EXECUTION PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5ASaBnvJune"
      },
      "outputs": [],
      "source": [
        "def askNLI(model, questions, answers, prompt_index, context_hypo_prompt = False, two_options_nli=False):\n",
        "\n",
        "  if model == 'BART':\n",
        "    results = askBartNLI(questions, answers, prompt_index, context_hypo_prompt, two_options_nli)\n",
        "  elif model == 'FLAN':\n",
        "    results = askFlanNLI(questions, answers, prompt_index, context_hypo_prompt, two_options_nli)\n",
        "  elif model == 'GPT2':\n",
        "    results = askGPT2NLI(questions, answers, prompt_index, context_hypo_prompt, two_options_nli)\n",
        "  elif model == 'GPT3':\n",
        "    results = askGPT3NLI(questions, answers, prompt_index, context_hypo_prompt, two_options_nli)\n",
        "  else:\n",
        "    results = askBartNLI(questions, answers, prompt_index, context_hypo_prompt, two_options_nli)\n",
        "\n",
        "  return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzsDRgeEmDGp"
      },
      "outputs": [],
      "source": [
        "NLI_DATASETS[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmFxDCnwicNO"
      },
      "outputs": [],
      "source": [
        "str = \"\";\n",
        "def test_on_datasets(model, test_mode = False):\n",
        "  binary_task = False;\n",
        "  for dataset_index in range (1, len(NLI_DATASETS)):\n",
        "    if dataset_index == 0:\n",
        "      binary_task = True;\n",
        "      single_sentence_propmts_num = 0;\n",
        "      prompt_num = len(NLI_TRUE_FALSE_PROMPTS);\n",
        "    else:\n",
        "      binary_task = False;\n",
        "      single_sentence_propmts_num = len(NLI_PROMPTS_SINGLE_SENTENCE)\n",
        "      prompt_num = single_sentence_propmts_num+len(NLI_PROMPT_CONTEXT_HYPO);\n",
        "\n",
        "    prompt_to_use = range(0, prompt_num) if test_mode == False else best_prompts[model][dataset_index]\n",
        "    for prompt_index in prompt_to_use:\n",
        "      question, answer = import_dataset(dataset_index, test_mode)\n",
        "\n",
        "      if dataset_index == 2 and test_mode:\n",
        "        question = question[0:1000]\n",
        "        answer = answer[0:1000]\n",
        "\n",
        "      context_hypo_prompt = prompt_index >= single_sentence_propmts_num\n",
        "\n",
        "      prompt_index = prompt_index - single_sentence_propmts_num if context_hypo_prompt else prompt_index;\n",
        "\n",
        "      results = askNLI(model, question, answer, prompt_index, context_hypo_prompt, binary_task)\n",
        "\n",
        "      dataset_name = NLI_DATASETS[dataset_index]\n",
        "\n",
        "      corrects = sum(bool(x) for x in results)\n",
        "\n",
        "      str = f\"Model: {model}\\tDataset name: {dataset_name}\\t Prompt with context: {context_hypo_prompt}\\t Prompt index: {prompt_index}\\t - Correct Answers {corrects}/{len(results)} - {corrects/len(results)}\"\n",
        "      print(str)\n",
        "      corrects = 0\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5-FkB0GPiK1"
      },
      "outputs": [],
      "source": [
        "question, answer = import_dataset(2, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH9xn2CUbH7w"
      },
      "source": [
        "### PROMPT SELECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGK10W2BkiqC"
      },
      "outputs": [],
      "source": [
        "test_on_datasets('FLAN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWanHQdbwPCd"
      },
      "outputs": [],
      "source": [
        "test_on_datasets('BART')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y__Ap5xko5h9"
      },
      "outputs": [],
      "source": [
        "test_on_datasets('GPT2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuyNP3G_W_TB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC8KCUHz9t1f"
      },
      "source": [
        "### EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bset_prompts = {}"
      ],
      "metadata": {
        "id": "R5C148YX61pU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzJI5Fre9w5f"
      },
      "outputs": [],
      "source": [
        "best_prompts = {\n",
        "    \"FLAN-SMALL\":[[ #best_prompts for \"scitail\"\n",
        "        1, 6, 8\n",
        "      ],[ #best_prompts for \"anli\"\n",
        "        6, 11, 15\n",
        "      ],[ #best_prompts for \"glue-mnli\"\n",
        "        9, 10, 11\n",
        "      ],[ #best_prompts for  \"sick\"\n",
        "        5, 6, 11\n",
        "      ],[ #best_prompts for \"superglue-cb\"\n",
        "        15, 16, 21\n",
        "      ]],\n",
        "      \"FLAN\":[[ #best_prompts for \"scitail\"\n",
        "        0, 1, 6\n",
        "      ],[ #best_prompts for \"anli\"\n",
        "        13, 15, 21\n",
        "      ],[ #best_prompts for \"glue-mnli\"\n",
        "        9, 11, 16\n",
        "      ],[ #best_prompts for  \"sick\"\n",
        "        14, 16, 20\n",
        "      ],[ #best_prompts for \"superglue-cb\"\n",
        "        12, #17, 18\n",
        "      ]],\n",
        "    \"BART\": [[ #best_prompts for \"scitail\"\n",
        "        1, 5, 8\n",
        "      ],[ #best_prompts for \"anli\"\n",
        "        2, 11, 21\n",
        "      ],[ #best_prompts for \"glue-mnli\"\n",
        "        2#, 12, 19\n",
        "      ],[ #best_prompts for  \"sick\"\n",
        "        1, 11, 18\n",
        "      ],[ #best_prompts for \"superglue-cb\"\n",
        "        1, 3, 20\n",
        "      ]],\n",
        "    \"GPT2\": [[ #best_prompts for \"scitail\"\n",
        "        1, 5, 8\n",
        "      ],[ #best_prompts for \"anli\"\n",
        "        9, 18, 21\n",
        "      ],[ #best_prompts for \"glue-mnli\"\n",
        "        4, 15, 19\n",
        "      ],[ #best_prompts for  \"sick\"\n",
        "        4, 14, 19\n",
        "      ],[ #best_prompts for \"superglue-cb\"\n",
        "        1, 3, 4\n",
        "      ]],\n",
        "    \"GPT3\": [[ #best_prompts for \"scitail\n",
        "\n",
        "      ],[ #best_prompts for \"anli\"\n",
        "\n",
        "      ],[ #best_prompts for \"glue-mnli\"\n",
        "\n",
        "      ],[ #best_prompts for  \"sick\"\n",
        "\n",
        "      ],[ #best_prompts for \"superglue-cb\"\n",
        "\n",
        "    ]]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek-TjWAedx6O"
      },
      "source": [
        "### BINARY NLI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOx_b97L9yT4"
      },
      "outputs": [],
      "source": [
        "test_on_datasets('FLAN', True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ETIhS2L9yRm"
      },
      "outputs": [],
      "source": [
        "test_on_datasets('GPT2', True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqEfOVH09yO-"
      },
      "outputs": [],
      "source": [
        "test_on_datasets('BART', True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgiPybKW9yJG"
      },
      "outputs": [],
      "source": [
        "test_on_datasets('BART', True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwKs5HZhd1Dd"
      },
      "source": [
        "### NLI (3 OPTIONS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTocrjBokabk"
      },
      "source": [
        "#### ANLI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "57zCI8nL9x1X"
      },
      "outputs": [],
      "source": [
        "test_on_datasets('FLAN', True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Te5FkYRgdxKK"
      },
      "outputs": [],
      "source": [
        "test_on_datasets('BART', True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1dQJa8uRmtS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7eHZ8wyckdpe"
      },
      "outputs": [],
      "source": [
        "test_on_datasets('GPT2', True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YouEo97tkpZa"
      },
      "source": [
        "#### GLUE-MNLI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNOXkJvpRp94"
      },
      "outputs": [],
      "source": [
        "test_on_datasets('FLAN', True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBPWTZq6U_wh"
      },
      "outputs": [],
      "source": [
        "test_on_datasets('GPT2', True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi5JYV3FkoGS"
      },
      "source": [
        "#### SICK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxcvbMfJ0x9J"
      },
      "outputs": [],
      "source": [
        "test_on_datasets('BART', True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgxCwOWx0ukz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M7e7swqkjKj"
      },
      "source": [
        "#### SUPEGLUE-CB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuhGkCZRkmVZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6dszq-B_nA4"
      },
      "source": [
        "### TIME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvgnpJUfACvf"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnqI0Es6BDeZ"
      },
      "outputs": [],
      "source": [
        "def time_tests(model, test_mode = True):\n",
        "  binary_task = False;\n",
        "  for dataset_index in range (0, len(NLI_DATASETS)):\n",
        "    if dataset_index == 0:\n",
        "      binary_task = True;\n",
        "      single_sentence_propmts_num = 0;\n",
        "      prompt_num = len(NLI_TRUE_FALSE_PROMPTS);\n",
        "    else:\n",
        "      binary_task = False;\n",
        "      single_sentence_propmts_num = len(NLI_PROMPTS_SINGLE_SENTENCE)\n",
        "      prompt_num = single_sentence_propmts_num+len(NLI_PROMPT_CONTEXT_HYPO);\n",
        "\n",
        "    prompt_index = best_prompts[model][dataset_index][0]\n",
        "    question, answer = import_dataset(dataset_index, test_mode)\n",
        "\n",
        "    if test_mode:\n",
        "      question = question[0:32]\n",
        "      answer = answer[0:32]\n",
        "\n",
        "    context_hypo_prompt = prompt_index >= single_sentence_propmts_num\n",
        "\n",
        "    prompt_index = prompt_index - single_sentence_propmts_num if context_hypo_prompt else prompt_index;\n",
        "\n",
        "    start = time.time()\n",
        "    results = askNLI(model, question, answer, prompt_index, context_hypo_prompt, binary_task)\n",
        "    end = time.time()\n",
        "\n",
        "    dataset_name = NLI_DATASETS[dataset_index]\n",
        "\n",
        "    str = f\"Model: {model}\\tDataset name: {dataset_name}\\t - Time: {end - start}\"\n",
        "    print(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGNpD1EW_rXi"
      },
      "outputs": [],
      "source": [
        "time_tests(\"FLAN\", True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79idhN3cEqFN"
      },
      "outputs": [],
      "source": [
        "time_tests(\"GPT2\", True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMNNTKceFA2m"
      },
      "outputs": [],
      "source": [
        "time_tests(\"BART\", True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "nJPj9uo4aNFW",
        "MDKP-1uGIi4P",
        "taSnFAUBZpO7",
        "GdHS26IXJaBI",
        "LuxxN1OOUWb_",
        "2onzu89NUY6W",
        "Q8NebOc6DKg9",
        "aH9xn2CUbH7w"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}