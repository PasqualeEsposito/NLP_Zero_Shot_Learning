{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i14_8V_1VjC6"
      },
      "source": [
        "# QUESTION & ANSWER NOTEBOOK\n",
        "In this notebook, analysis and experiments on QA datasets are present. Specifically, tests on the following datasets have been conducted:\n",
        "- QuaRel\n",
        "- QuaRTz-no_knowledge\n",
        "- QuaRTz-with_knowledge\n",
        "- RACE-middle\n",
        "- SciQ\n",
        "- Social IQA\n",
        "- SuperGLUE COPA\n",
        "- Wino Grande"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfqkATZpWX97"
      },
      "source": [
        "## INITIALIZATION AND UTILITY FUNCTIONS DEFINITION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfjz0ETbFu-H"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vg9alhKIGwKz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCbQY61FBrgk"
      },
      "outputs": [],
      "source": [
        "path_prefix = \"/content/gdrive/MyDrive/DATASETS/QA/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwLwX7abBrcD"
      },
      "outputs": [],
      "source": [
        "cd /content/gdrive/MyDrive/DATASETS/QA/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvdOQyr-BrYv"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCLLXtwHBrV6"
      },
      "outputs": [],
      "source": [
        "####################################################### DA FARE IL MERGE\n",
        "QA_DATASETS = [\n",
        "    # {\n",
        "    #     \"name\": \"ai2_arc\", #ok\n",
        "    #     \"sep\": False,\n",
        "    #     \"initial_premise\": False,\n",
        "    #     \"end_premise\": False,\n",
        "    #     \"num_options\": 4,\n",
        "    # }, {\n",
        "    #     \"name\": \"aqua_rat\", #ok\n",
        "    #     \"sep\": False,\n",
        "    #     \"initial_premise\": False,\n",
        "    #     \"end_premise\": False,\n",
        "    #     \"num_options\": 5\n",
        "    # }, {\n",
        "    #     \"name\": \"codah\", #ok\n",
        "    #     \"sep\": False,\n",
        "    #     \"initial_premise\": False,\n",
        "    #     \"end_premise\": False,\n",
        "    #     \"num_options\": 4\n",
        "    # }, {\n",
        "    #     \"name\": \"commonsense_qa\", #ok\n",
        "    #     \"sep\": False,\n",
        "    #     \"initial_premise\": False,\n",
        "    #     \"end_premise\": False,\n",
        "    #     \"num_options\": 5\n",
        "    # }, {\n",
        "    #     \"name\": \"cosmos_qa\", #ok\n",
        "    #     \"sep\": True,\n",
        "    #     \"initial_premise\": True,\n",
        "    #     \"end_premise\": False,\n",
        "    #     \"num_options\": 4\n",
        "    # }, {\n",
        "    #     \"name\": \"dream\", #ok\n",
        "    #     \"sep\": True,\n",
        "    #     \"initial_premise\": True,\n",
        "    #     \"end_premise\": False,\n",
        "    #     \"num_options\": 3\n",
        "    # }, {\n",
        "    #     \"name\": \"math_qa\", #ok\n",
        "    #     \"sep\": False,\n",
        "    #     \"initial_premise\": False,\n",
        "    #     \"end_premise\": False,\n",
        "    #     \"num_options\": 5\n",
        "    # }, {\n",
        "    #     \"name\": \"openbookqa\", #ok\n",
        "    #     \"sep\": False,\n",
        "    #     \"initial_premise\": False,\n",
        "    #     \"end_premise\": False,\n",
        "    #     \"num_options\": 4\n",
        "    # }, {\n",
        "    #     \"name\": \"qasc\", #ok\n",
        "    #     \"sep\": False,\n",
        "    #     \"initial_premise\": False,\n",
        "    #     \"end_premise\": False,\n",
        "    #     \"num_options\": 8\n",
        "    # }, {\n",
        "    #     \"name\": \"quail\", #ok\n",
        "    #     \"sep\": True,\n",
        "    #     \"initial_premise\": False,\n",
        "    #     \"end_premise\": True,\n",
        "    #     \"num_options\": 4\n",
        "    # },\n",
        "     {\n",
        "        \"name\": \"quarel\", #ok\n",
        "        \"sep\": False,\n",
        "        \"initial_premise\": False,\n",
        "        \"end_premise\": False,\n",
        "        \"num_options\": 2,\n",
        "        \"FLAN\": [0, 4, 13],\n",
        "        #\"FLAN\": [2, 4, 13], small\n",
        "        \"GPT2\": [3, 10, 11],\n",
        "        \"BART\": [6, 9, 10]\n",
        "    }, {\n",
        "        \"name\": \"quartz-no_knowledge\", #ok\n",
        "        \"sep\": False,\n",
        "        \"initial_premise\": False,\n",
        "        \"end_premise\": False,\n",
        "        \"num_options\": 2,\n",
        "        \"FLAN\": [1, 2, 3],\n",
        "        #\"FLAN\": [1, 6, 13], small\n",
        "        \"GPT2\": [1, 5, 10],\n",
        "        \"BART\": [2, 3, 6]\n",
        "    },{\n",
        "        \"name\": \"quartz-with_knowledge\", #ok\n",
        "        \"sep\": False,\n",
        "        \"initial_premise\": False,\n",
        "        \"end_premise\": False,\n",
        "        \"num_options\": 2,\n",
        "        \"FLAN\": [0, 1, 3],\n",
        "        #\"FLAN\": [1, 2, 5], small\n",
        "        \"GPT2\": [7, 10, 14],\n",
        "        \"BART\": [11, 13, 14]\n",
        "    },\n",
        "    # {\n",
        "    #     \"name\": \"race-high\", #ok\n",
        "    #     \"sep\": True,\n",
        "    #     \"initial_premise\": False,\n",
        "    #     \"end_premise\": True,\n",
        "    #     \"num_options\": 4\n",
        "    # },\n",
        "     {\n",
        "        \"name\": \"race-middle\", #ok\n",
        "        \"sep\": True,\n",
        "        \"initial_premise\": False,\n",
        "        \"end_premise\": True,\n",
        "        \"num_options\": 4,\n",
        "        \"FLAN\": [7],#[5, 2, 7],\n",
        "        #\"FLAN\": [3, 6, 7], small\n",
        "        \"GPT2\": [5, 10, 13],\n",
        "        \"BART\": [7, 10, 11]\n",
        "    }, {\n",
        "        \"name\": \"sciq\", #ok\n",
        "        \"sep\": True,\n",
        "        \"initial_premise\": False,\n",
        "        \"end_premise\": True,\n",
        "        \"num_options\": 4,\n",
        "        \"FLAN\": [1, 2, 3],\n",
        "        #\"FLAN\": [2, 6, 8], small\n",
        "        \"GPT2\": [5, 8, 14],\n",
        "        \"BART\": [9, 12, 14]\n",
        "    }, {\n",
        "        \"name\": \"social_i_qa\", #ok\n",
        "        \"sep\": True,\n",
        "        \"initial_premise\": False,\n",
        "        \"end_premise\": True,\n",
        "        \"num_options\": 3,\n",
        "        \"FLAN\": [2, 7, 8],\n",
        "        #\"FLAN\": [1, 4, 8], small\n",
        "        \"GPT2\": [1, 4, 14],\n",
        "        \"BART\": [0, 2, 4]\n",
        "    }, {\n",
        "        \"name\": \"superglue-copa\", #ok\n",
        "        \"sep\": False,\n",
        "        \"initial_premise\": False,\n",
        "        \"end_premise\": False,\n",
        "        \"num_options\": 2,\n",
        "        \"FLAN\": [0, 3, 4],\n",
        "        #\"FLAN\": [2, 5, 8], small\n",
        "        \"GPT2\": [7, 10, 12],\n",
        "        \"BART\": [0, 1, 4]\n",
        "    },\n",
        "    # {\n",
        "    #     \"name\": \"superglue-multirc\", #ok\n",
        "    #     \"sep\": \"context:\",\n",
        "    #     \"initial_premise\": False,\n",
        "    #     \"end_premise\": True,\n",
        "    #     \"num_options\": 5\n",
        "    # }, {\n",
        "    #     \"name\": \"swag\", #ok\n",
        "    #     \"sep\": False,\n",
        "    #     \"initial_premise\": False,\n",
        "    #     \"end_premise\": False,\n",
        "    #     \"num_options\": 4\n",
        "    # },\n",
        "     {\n",
        "        \"name\": \"wino_grande\", #ok\n",
        "        \"sep\": False,\n",
        "        \"initial_premise\": False,\n",
        "        \"end_premise\": False,\n",
        "        \"num_options\": 2,\n",
        "        \"FLAN\": [7, 11, 13],\n",
        "        #\"FLAN\": [1, 10, 11], small\n",
        "        \"GPT2\": [9, 11, 13],\n",
        "        \"BART\": [5, 9, 12]\n",
        "    },\n",
        "    # {\n",
        "    #     \"name\": \"wiqa\", #ok\n",
        "    #     \"sep\": True,\n",
        "    #     \"initial_premise\": False,\n",
        "    #     \"end_premise\": True,\n",
        "    #     \"num_options\": 3\n",
        "    # }\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11FhBQapHJ06"
      },
      "outputs": [],
      "source": [
        "QA_PROMPTS = [\n",
        "    \"For the following Question Answering task choose the correct answer between the given options. Question: {QUESTION}. Options: {OPTIONS}\",\n",
        "    \"Given the following Question Answering task, choose an answer between the options. Question: {QUESTION}. Options: {OPTIONS}\",\n",
        "    \"Give an answer for the following Question Answering task. Question: {QUESTION}. Options: {OPTIONS}\",\n",
        "    \"Which is the correct answer among the options? Question: {QUESTION}. Options: {OPTIONS}\",\n",
        "    \"Choose the correct answer among the options. Question: {QUESTION}. Options: {OPTIONS}\",\n",
        "    \"Given these options {OPTIONS}, answer the following question: {QUESTION}. Answer using only the words present in the options.\",\n",
        "    \"Answer the following question:\\n\\n{QUESTION}\\n\\nOptions: {OPTIONS}\",\n",
        "    \"Answer this question:\\n\\n{QUESTION}?\\nOptions: {OPTIONS}\",\n",
        "    \"What is the answer to this question? {QUESTION}\\nOptions: {OPTIONS}\",\n",
        "    \"What is the answer to the following: {QUESTION}? {OPTIONS}\",\n",
        "    \"Provide the answer to: {QUESTION}. {OPTIONS}\",\n",
        "    \"Answer the following: {QUESTION}. {OPTIONS}\",\n",
        "    \"What is the response to: {QUESTION}? {OPTIONS}\",\n",
        "    \"In the following text: {QUESTION}, what is the answer? {OPTIONS}\",\n",
        "    \"Given the input {QUESTION}, provide the answer. {OPTIONS}\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgIkoty_D32L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DJnJzUeKDBV"
      },
      "outputs": [],
      "source": [
        "len(QA_DATASETS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FfEACq0HQtj"
      },
      "outputs": [],
      "source": [
        "DATASET_SEEDS = [13, 21, 42, 87, 100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of39tBHsHWd7"
      },
      "outputs": [],
      "source": [
        "def import_dataset(dataset_index):\n",
        "  dataset_info = QA_DATASETS[dataset_index];\n",
        "\n",
        "  dataset_name = dataset_info[\"name\"]\n",
        "  print(dataset_name)\n",
        "  #for seed in DATASET_SEEDS:\n",
        "  seed = 100 #test_set is equal independently on random seed\n",
        "  test_set_path = f\"{dataset_name}/{dataset_name}_32_{seed}_test.tsv\"\n",
        "  #print(test_set_path)\n",
        "  df = pd.read_csv(test_set_path , sep=\"\\t\", header=None)\n",
        "\n",
        "  has_sep = dataset_info[\"sep\"]\n",
        "  has_initial_premise = dataset_info[\"initial_premise\"]\n",
        "  has_end_premise = dataset_info[\"end_premise\"]\n",
        "\n",
        "  if(has_sep and has_initial_premise):\n",
        "    df[0] = df[0].map(handle_initial_sep)\n",
        "\n",
        "  if(has_sep and has_end_premise):\n",
        "    if(has_sep != True):\n",
        "      df[0] = df[0].apply(lambda q: handle_end_sep(q, has_sep, [\"question:\"]))\n",
        "    else:\n",
        "      df[0] = df[0].map(handle_end_sep)\n",
        "  #print(df[0][6])\n",
        "  return (df[0], df[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jcKXG-nvbHi"
      },
      "outputs": [],
      "source": [
        "def handle_initial_sep(query, separator = \"\\[SEP\\]\"):\n",
        "  qpa = re.split(separator, query) #Vector containing in this order QUESTION, PREMISE and ANSWER.\n",
        "  return f\"{qpa[1]}{qpa[0]}{qpa[2]}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_R7ALBa9JKtE"
      },
      "outputs": [],
      "source": [
        "def handle_end_sep(query, separator = \"\\[SEP\\]\", to_remove=[]):\n",
        "  if (len(to_remove) != 0):\n",
        "    for substr in to_remove:\n",
        "      query = query.replace(substr, '')\n",
        "\n",
        "  qap = re.split(separator, query)\n",
        "\n",
        "  return f\"{qap[1]}{qap[0]}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnr2aOW5HCZg"
      },
      "outputs": [],
      "source": [
        "for dataset_index in range (0, len(QA_DATASETS)):\n",
        "  import_dataset(dataset_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0z931m7jEqI"
      },
      "outputs": [],
      "source": [
        "def preprocess(premises):\n",
        "  premises.tolist()\n",
        "  question = []\n",
        "  answer = []\n",
        "\n",
        "  for premise in premises:\n",
        "    split = re.split(\"\\([A-H]\\)\", premise)\n",
        "    question.append(split.pop(0))\n",
        "    answer.append(split)\n",
        "\n",
        "  return question, answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhMXNjV3JWPk"
      },
      "outputs": [],
      "source": [
        "def adapt_prompt(prompt_index, question, options):\n",
        "  return QA_PROMPTS[prompt_index].format(QUESTION = question, OPTIONS = options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZw2BtrOXQgz"
      },
      "source": [
        "## MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prW904b-c3L8"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6yj8pjsVSE9"
      },
      "source": [
        "### FLANT5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w66SlaIbbrzo"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud7HZIRWWoX0"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0Oe0FqUWoX1"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "flan_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "flan_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Uox9JZhWoX2"
      },
      "outputs": [],
      "source": [
        "def askFlanQA(data, solutions, prompt_index = None):\n",
        "  q, a = preprocess(data)\n",
        "  results = []\n",
        "  solutions = solutions.tolist()\n",
        "\n",
        "  for question, options, solution in zip(q, a, solutions):\n",
        "    if(prompt_index != None):\n",
        "      prompt = adapt_prompt(prompt_index, question, options)\n",
        "    else:\n",
        "      prompt = question\n",
        "    #print(prompt)\n",
        "    input_ids = flan_tokenizer(prompt, return_tensors=\"pt\",truncation=True, padding=True, max_length=1024)\n",
        "\n",
        "    output_sequences = flan_model.generate(input_ids=input_ids[\"input_ids\"], max_length=1000)\n",
        "\n",
        "    result = flan_tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n",
        "\n",
        "    #print(flan_tokenizer.batch_decode(output_sequences, skip_special_tokens=True)[0].replace(\" \", \"\").replace(\".\", \"\").lower())\n",
        "    #print(solution.replace(\" \", \"\").replace(\".\", \"\").lower())\n",
        "\n",
        "    #print(f\"FLAN answer: {result}, correct answer: {solution}\")\n",
        "\n",
        "    if(flan_tokenizer.batch_decode(output_sequences, skip_special_tokens=True)[0].replace(\" \", \"\").replace(\".\", \"\").lower() == solution.replace(\" \", \"\").replace(\".\", \"\").lower()):\n",
        "      results.append(True)\n",
        "    else:\n",
        "      results.append(False)\n",
        "\n",
        "    #results.append(flan_tokenizer.batch_decode(output_sequences, skip_special_tokens=True))\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od2_B4v3WJDW"
      },
      "source": [
        "### BART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Di7PzbOz_AzK"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "bart_classifier = pipeline(\"zero-shot-classification\",\n",
        "                      model=\"facebook/bart-large-mnli\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiMcfDIqxRKy"
      },
      "outputs": [],
      "source": [
        "def askBartQA(data, solutions, prompt_index = None):\n",
        "  question, answers = preprocess(data)\n",
        "  solutions = solutions.tolist()\n",
        "  results = []\n",
        "\n",
        "  for i in range (0, len(question)):\n",
        "    if(prompt_index != None):\n",
        "      prompt = adapt_prompt(prompt_index, question[i], answers[i])\n",
        "    else:\n",
        "      data[i]\n",
        "    BART_answer = bart_classifier(prompt, answers[i])[\"labels\"][0]\n",
        "    #print(f\"BART answer: {BART_answer}, correct answer: {solutions[i]}\")\n",
        "    if(BART_answer.replace(\" \", \"\").lower() == solutions[i].replace(\" \", \"\").lower()):\n",
        "      results.append(True)\n",
        "    else:\n",
        "      results.append(False)\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgjqDIUiVYGE"
      },
      "source": [
        "### GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4F3GWCiokHc"
      },
      "outputs": [],
      "source": [
        "# Check if larger versions exist and can be run.\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIbK7KaTmgGi"
      },
      "outputs": [],
      "source": [
        "from string import Template\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "#import streamlit as st\n",
        "\n",
        "def gpt2_classify(inputs, candidate_labels, prompt=\"$input is a type of \"):\n",
        "\n",
        "    if type(candidate_labels) == list:\n",
        "        candidate_labels = {k: [k] for k in candidate_labels}\n",
        "\n",
        "    # Add the final space of the prompt to each candidate label\n",
        "    if prompt[-1] == \" \":\n",
        "        content_prefix, prompt = \" \", prompt[:-1]\n",
        "    else:\n",
        "        content_prefix = \"\"\n",
        "\n",
        "    # Encode the part of the prompt which is independent of the inputs\n",
        "    prompt_prefix = prompt.split(\"$input\")[0]\n",
        "    if len(prompt_prefix) > 0:\n",
        "        prompt = prompt[len(prompt_prefix) :]\n",
        "        if prompt_prefix[-1] == \" \":\n",
        "            prompt_prefix, prompt = prompt_prefix[:-1], \" \" + prompt\n",
        "        inputs_prefix = gpt2_tokenizer(prompt_prefix, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            past_key_values_prefix = gpt2_model(**inputs_prefix).past_key_values\n",
        "        del inputs_prefix\n",
        "\n",
        "    scores = []\n",
        "    # For each input, encode the input-dependent part of the prompt\n",
        "    for input in inputs:\n",
        "        inputs = gpt2_tokenizer.encode(Template(prompt).substitute(input=input))\n",
        "        inputs = torch.tensor([inputs])\n",
        "        with torch.no_grad():\n",
        "            if len(prompt_prefix) > 0:\n",
        "                outputs = gpt2_model(inputs, past_key_values=past_key_values_prefix)\n",
        "            else:\n",
        "                outputs = gpt2_model(inputs)\n",
        "        del inputs\n",
        "        probs = torch.softmax(outputs.logits[0, -1, :], -1).detach().cpu().numpy()\n",
        "        past_key_values = outputs.past_key_values\n",
        "        del outputs\n",
        "\n",
        "        scores.append({\"sequence\": input})\n",
        "        scores[-1][\"labels\"] = [k for k in candidate_labels]\n",
        "        scores[-1][\"scores\"] = []\n",
        "        # Get the probability of each candidate label being the next tokens\n",
        "        for k in candidate_labels:\n",
        "            scores[-1][\"scores\"].append(0)\n",
        "            for word in candidate_labels[k]:\n",
        "                tokens = gpt2_tokenizer.encode(content_prefix + word)\n",
        "                prob = probs[tokens[0]]\n",
        "                if len(tokens) > 1:\n",
        "                    with torch.no_grad():\n",
        "                        outputs2 = gpt2_model(\n",
        "                            torch.tensor([tokens[:-1]]),\n",
        "                            past_key_values=past_key_values,\n",
        "                        )\n",
        "                    probs2 = (\n",
        "                        torch.softmax(outputs2.logits[0, :, :], -1)\n",
        "                        .detach()\n",
        "                        .cpu()\n",
        "                        .numpy()\n",
        "                    )\n",
        "                    del outputs2\n",
        "                    for i in range(1, len(tokens)):\n",
        "                        prob *= probs2[i - 1, tokens[i]]\n",
        "                scores[-1][\"scores\"][-1] += prob\n",
        "        # Normalize the scores to get probabilities\n",
        "        sum_scores = sum(scores[-1][\"scores\"])\n",
        "        scores[-1][\"scores\"] = [x / sum_scores for x in scores[-1][\"scores\"]]\n",
        "\n",
        "    return scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AJpndjhErT3"
      },
      "outputs": [],
      "source": [
        "def askGPT2QA(premise, solutions, prompt_index = None):\n",
        "\n",
        "  res = []\n",
        "  results = []\n",
        "  solutions = solutions.tolist()\n",
        "\n",
        "  premise, answers = preprocess(premise)\n",
        "\n",
        "  for i in range (0, len(premise)):\n",
        "    if(prompt_index != None):\n",
        "      prompt = adapt_prompt(prompt_index, premise[i], answers[i])\n",
        "    else:\n",
        "      premise[i]\n",
        "    res = gpt2_classify([prompt], answers[i])\n",
        "    if(res[0][\"labels\"][res[0][\"scores\"].index(max(res[0][\"scores\"]))].replace(\" \", \"\").replace(\".\", \"\").lower() == solutions[i].replace(\" \", \"\").replace(\".\", \"\").lower()):\n",
        "      results.append(True)\n",
        "    else:\n",
        "      results.append(False)\n",
        "\n",
        "    #print(\"GPT2 answer:\", res[0][\"labels\"][res[0][\"scores\"].index(max(res[0][\"scores\"]))].replace(\" \", \"\").replace(\".\", \"\").lower())\n",
        "    #print(\"Correct answer:\", solutions[i].replace(\" \", \"\").replace(\".\", \"\").lower())\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxMDk5hEWHk6"
      },
      "source": [
        "### GPT3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Sd8Rukqsy9e"
      },
      "outputs": [],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AozewMcCs40g"
      },
      "outputs": [],
      "source": [
        "import openai as ai\n",
        "\n",
        "ai.api_key = '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' #chiave di Pasquale\n",
        "#ai.api_key = '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' # replace with your key from earlier, chiave di Sergio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO0Dj9QFul4B"
      },
      "outputs": [],
      "source": [
        "def generate_gpt3_response(user_text, print_output=False):\n",
        "    \"\"\"\n",
        "    Query OpenAI GPT-3 for the specific key and get back a response\n",
        "    :type user_text: str the user's text to query for\n",
        "    :type print_output: boolean whether or not to print the raw output JSON\n",
        "    \"\"\"\n",
        "    completions = ai.Completion.create(\n",
        "        engine='text-davinci-003',  # Determines the quality, speed, and cost.\n",
        "        temperature=0,              # Level of creativity in the response\n",
        "        prompt=user_text,           # What the user typed in\n",
        "        max_tokens=100,             # Maximum tokens in the prompt AND response\n",
        "        n=1,                        # The number of completions to generate\n",
        "        stop=None,                  # An optional setting to control response generation\n",
        "    )\n",
        "\n",
        "    # Displaying the output can be helpful if things go wrong\n",
        "    if print_output:\n",
        "        print(completions)\n",
        "\n",
        "    # Return the first choice's text\n",
        "    return completions.choices[0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPiSHU7B88p7"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from random import shuffle\n",
        "random.seed(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5jjVy4DYI_h"
      },
      "outputs": [],
      "source": [
        "def askGPT3QA(data, solutions, prompt_index = None):\n",
        "  results = []\n",
        "  q, a = preprocess(data)\n",
        "  solutions = solutions.tolist()\n",
        "\n",
        "  for question, options, solution in zip(q, a, solutions):\n",
        "      if(prompt_index != None):\n",
        "        prompt = adapt_prompt(prompt_index, question, options)\n",
        "\n",
        "      #print(prompt)\n",
        "\n",
        "      #print(generate_gpt3_response(prompt).replace(\"\\n\\n\", \"\")) ## TODO check if answer is correct in a second moment.\n",
        "\n",
        "      #print(\"GPT3 answer:\", generate_gpt3_response(prompt).replace(\"\\n\\n\", \"\").replace(\" \", \"\").replace(\".\", \"\").lower())\n",
        "      #print(\"Correct answer:\", solution.replace(\" \", \"\").replace(\".\", \"\").lower())\n",
        "\n",
        "      if(generate_gpt3_response(prompt).replace(\"\\n\\n\", \"\").replace(\" \", \"\").replace(\".\", \"\").lower() == solution.replace(\" \", \"\").replace(\".\", \"\").lower()):\n",
        "        results.append(True)\n",
        "      else:\n",
        "        results.append(False)\n",
        "\n",
        "      #print(results)\n",
        "\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5VWDU0yINVx"
      },
      "outputs": [],
      "source": [
        "def askQA(model, questions, answers, prompt_index = None):\n",
        "\n",
        "  if model == 'BART':\n",
        "    results = askBartQA(questions, answers, prompt_index)\n",
        "  elif model == 'FLAN':\n",
        "    results = askFlanQA(questions, answers, prompt_index) ## TODO change\n",
        "  elif model == 'GPT2':\n",
        "    results = askGPT2QA(questions, answers, prompt_index) ## TODO change\n",
        "  elif model == 'GPT3':\n",
        "    results = askGPT3QA(questions, answers, prompt_index) ## TODO change\n",
        "  else:\n",
        "    results = askBartQA(questions, answers, prompt_index)\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mqgh3XMmIPjj"
      },
      "outputs": [],
      "source": [
        "def test_on_datasets(model, prompt_index = None):\n",
        "  for dataset_index in range (9, 10):#len(QA_DATASETS)):\n",
        "    question, answer = import_dataset(dataset_index)\n",
        "    results = askQA(model, question[0:32], answer[0:32], prompt_index)\n",
        "\n",
        "    dataset_name = QA_DATASETS[dataset_index][\"name\"]\n",
        "\n",
        "    corrects = sum(bool(x) for x in results)\n",
        "    print(f\"{dataset_name} - Correct Answers {corrects}/{len(results)} - {corrects/len(results)}\" )\n",
        "    corrects = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRzPQ_8cHTAu"
      },
      "source": [
        "## CHOICE OF THE PROMPTS TO USE FOR EACH DATASET\n",
        "In the following the prompt selection phase is shown. We take the validation set of each considered dataset and we iteratively check the performance of the defined prompts in order to pick the prompt that better performs for each dataset and model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOD2dPzXHdZs"
      },
      "outputs": [],
      "source": [
        "#Modifico questa funzione così da caricare i file per il validation set\n",
        "def import_dataset(dataset_index):\n",
        "  dataset_info = QA_DATASETS[dataset_index];\n",
        "\n",
        "  dataset_name = dataset_info[\"name\"]\n",
        "  print(dataset_name)\n",
        "  #for seed in DATASET_SEEDS:\n",
        "  seed = 100 #test_set is equal independently on random seed\n",
        "  test_set_path = f\"{dataset_name}/{dataset_name}_32_{seed}_dev.tsv\"\n",
        "  #print(test_set_path)\n",
        "  df = pd.read_csv(test_set_path , sep=\"\\t\", header=None)\n",
        "\n",
        "  has_sep = dataset_info[\"sep\"]\n",
        "  has_initial_premise = dataset_info[\"initial_premise\"]\n",
        "  has_end_premise = dataset_info[\"end_premise\"]\n",
        "\n",
        "  if(has_sep and has_initial_premise):\n",
        "    df[0] = df[0].map(handle_initial_sep)\n",
        "\n",
        "  if(has_sep and has_end_premise):\n",
        "    if(has_sep != True):\n",
        "      df[0] = df[0].apply(lambda q: handle_end_sep(q, has_sep, [\"question:\"]))\n",
        "    else:\n",
        "      df[0] = df[0].map(handle_end_sep)\n",
        "  #print(df[0][6])\n",
        "  return (df[0], df[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVRi51sYJHoa"
      },
      "source": [
        "### TEST FOR PROMPTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDP3nFMVI_gS"
      },
      "outputs": [],
      "source": [
        "def askQA(model, questions, answers, prompt_index = None):\n",
        "\n",
        "  if model == 'BART':\n",
        "    results = askBartQA(questions, answers, prompt_index)\n",
        "  elif model == 'FLAN':\n",
        "    results = askFlanQA(questions, answers, prompt_index) ## TODO change\n",
        "  elif model == 'GPT2':\n",
        "    results = askGPT2QA(questions, answers, prompt_index) ## TODO change\n",
        "  elif model == 'GPT3':\n",
        "    results = askGPT3QA(questions, answers, prompt_index) ## TODO change\n",
        "  else:\n",
        "    results = askBartQA(questions, answers, prompt_index)\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE6W7sHkMtdM"
      },
      "outputs": [],
      "source": [
        "def import_dataset(dataset_index):\n",
        "  dataset_info = QA_DATASETS[dataset_index];\n",
        "\n",
        "  dataset_name = dataset_info[\"name\"]\n",
        "  #print(dataset_name) HO COMMENTATO QUESTA STRINGA, NON HO MODIFICATO NIENT'ALTRO\n",
        "  #for seed in DATASET_SEEDS:\n",
        "  seed = 100 #test_set is equal independently on random seed\n",
        "  test_set_path = f\"{dataset_name}/{dataset_name}_32_{seed}_dev.tsv\"\n",
        "  #print(test_set_path)\n",
        "  df = pd.read_csv(test_set_path , sep=\"\\t\", header=None)\n",
        "\n",
        "  has_sep = dataset_info[\"sep\"]\n",
        "  has_initial_premise = dataset_info[\"initial_premise\"]\n",
        "  has_end_premise = dataset_info[\"end_premise\"]\n",
        "\n",
        "  if(has_sep and has_initial_premise):\n",
        "    df[0] = df[0].map(handle_initial_sep)\n",
        "\n",
        "  if(has_sep and has_end_premise):\n",
        "    if(has_sep != True):\n",
        "      df[0] = df[0].apply(lambda q: handle_end_sep(q, has_sep, [\"question:\"]))\n",
        "    else:\n",
        "      df[0] = df[0].map(handle_end_sep)\n",
        "  #print(df[0][6])\n",
        "  return (df[0], df[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SloWdixJCfn"
      },
      "outputs": [],
      "source": [
        "str = \"\";\n",
        "def test_on_datasets():\n",
        "  for model in [\"FLAN\"]:\n",
        "    for dataset_index in range (0, len(QA_DATASETS)):\n",
        "      for prompt in range (0, len(QA_PROMPTS)):\n",
        "        question, answer = import_dataset(dataset_index)\n",
        "        results = askQA(model, question, answer, prompt)\n",
        "\n",
        "        dataset_name = QA_DATASETS[dataset_index][\"name\"]\n",
        "\n",
        "        corrects = sum(bool(x) for x in results)\n",
        "\n",
        "        str = f\"Model: {model}\\tDataset name: {dataset_name}\\tPrompt index: {prompt} - Correct Answers {corrects}/{len(results)} - {corrects/len(results)}\"\n",
        "        print(str)\n",
        "        corrects = 0\n",
        "      print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmIeq8-XJG69"
      },
      "outputs": [],
      "source": [
        "test_on_datasets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-_KW801XTsG"
      },
      "source": [
        "## EXPERIMENTS\n",
        "Final experiments performed using, for each dataset, the prompt that better performs with each considered model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k15LDoefhyu4"
      },
      "outputs": [],
      "source": [
        "def askQA(model, questions, answers, prompt_index = None):\n",
        "\n",
        "  if model == 'BART':\n",
        "    results = askBartQA(questions, answers, prompt_index)\n",
        "  elif model == 'FLAN':\n",
        "    results = askFlanQA(questions, answers, prompt_index) ## TODO change\n",
        "  elif model == 'GPT2':\n",
        "    results = askGPT2QA(questions, answers, prompt_index) ## TODO change\n",
        "  elif model == 'GPT3':\n",
        "    results = askGPT3QA(questions, answers, prompt_index) ## TODO change\n",
        "  else:\n",
        "    results = askBartQA(questions, answers, prompt_index)\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddbABW5rhp1k"
      },
      "outputs": [],
      "source": [
        "#Modifico questa funzione così da caricare i file per il validation set\n",
        "def import_dataset(dataset_index):\n",
        "  dataset_info = QA_DATASETS[dataset_index];\n",
        "\n",
        "  dataset_name = dataset_info[\"name\"]\n",
        "  #print(dataset_name)\n",
        "  #for seed in DATASET_SEEDS:\n",
        "  seed = 100 #test_set is equal independently on random seed\n",
        "  test_set_path = f\"{dataset_name}/{dataset_name}_32_{seed}_test.tsv\"\n",
        "  #print(test_set_path)\n",
        "  df = pd.read_csv(test_set_path , sep=\"\\t\", header=None)\n",
        "\n",
        "  has_sep = dataset_info[\"sep\"]\n",
        "  has_initial_premise = dataset_info[\"initial_premise\"]\n",
        "  has_end_premise = dataset_info[\"end_premise\"]\n",
        "\n",
        "  if(has_sep and has_initial_premise):\n",
        "    df[0] = df[0].map(handle_initial_sep)\n",
        "\n",
        "  if(has_sep and has_end_premise):\n",
        "    if(has_sep != True):\n",
        "      df[0] = df[0].apply(lambda q: handle_end_sep(q, has_sep, [\"question:\"]))\n",
        "    else:\n",
        "      df[0] = df[0].map(handle_end_sep)\n",
        "  #print(df[0][6])\n",
        "  return (df[0], df[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wjeTk15fTDD"
      },
      "outputs": [],
      "source": [
        "def final_tests():\n",
        "    for dataset_index in range (0, 3):\n",
        "      for model in [\"FLAN\"]:\n",
        "        for prompt in QA_DATASETS[dataset_index][model]:\n",
        "          question, answer = import_dataset(dataset_index)\n",
        "\n",
        "          print(QA_DATASETS[dataset_index][\"name\"])\n",
        "          print(prompt)\n",
        "\n",
        "          results = askQA(model, question, answer, prompt)\n",
        "\n",
        "          dataset_name = QA_DATASETS[dataset_index][\"name\"]\n",
        "\n",
        "          corrects = sum(bool(x) for x in results)\n",
        "\n",
        "          str = f\"Model: {model}\\tDataset name: {dataset_name}\\tPrompt index: {prompt} - Correct Answers {corrects}/{len(results)} - {corrects/len(results)}\"\n",
        "          print(str)\n",
        "          corrects = 0\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMUubauIhlHt"
      },
      "outputs": [],
      "source": [
        "final_tests()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TIME SPENT FOR THE COMPUTATION"
      ],
      "metadata": {
        "id": "WOI1-oDiLwmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "wy3rzbYCLs1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def time_tests():\n",
        "    for dataset_index in range (0, len(QA_DATASETS)):\n",
        "      for model in [\"FLAN\"]:\n",
        "        question, answer = import_dataset(dataset_index)\n",
        "        start = time.time()\n",
        "        results = askQA(model, question[0:32], answer[0:32], QA_DATASETS[dataset_index][model][0])\n",
        "        end = time.time()\n",
        "\n",
        "        dataset_name = QA_DATASETS[dataset_index][\"name\"]\n",
        "        str = f\"Model: {model}\\tDataset name: {dataset_name}\\t - Time: {end - start}\"\n",
        "        print(str)\n",
        "      print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "49pWjD0tL-Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_tests()"
      ],
      "metadata": {
        "id": "lvImkR9qMwXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L_xPcwBi0ujs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "BZw2BtrOXQgz",
        "Od2_B4v3WJDW",
        "WgjqDIUiVYGE",
        "mxMDk5hEWHk6"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}