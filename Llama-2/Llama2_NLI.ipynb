{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kuzui8HlrU36"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!huggingface-cli login --token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeJd_DrLtjz_"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate\n",
        "!pip install xformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d9jzQdRm5Goy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJPj9uo4aNFW"
      },
      "source": [
        "## SETUP AND DATASET LOADING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeYCgPuV72aG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4cCxoS1HvT2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoMYJDpcHvRM"
      },
      "outputs": [],
      "source": [
        "path_prefix = \"/content/gdrive/MyDrive/DATASETS/NLI/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-Y6KJ8PHvOg"
      },
      "outputs": [],
      "source": [
        "cd /content/gdrive/MyDrive/DATASETS/NLI/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWUtm4a_HvMF"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt2Uo1NSHvHz"
      },
      "outputs": [],
      "source": [
        "NLI_DATASETS = [\"scitail\", \"anli\", \"glue-mnli\", \"sick\", \"superglue-cb\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KScehrvMSfVH"
      },
      "source": [
        "superglue cb\n",
        "sick\n",
        "scitail\n",
        "anli\n",
        "glue-mnli\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rjOy2xTJTyW"
      },
      "outputs": [],
      "source": [
        "NLI_DATASETS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Scz4sjfPHvFv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UEnQnb0JnU0"
      },
      "outputs": [],
      "source": [
        "DATASET_SEEDS = [13, 21, 42, 87, 100]\n",
        "WORDS_TO_REMOVE = [\"context:\", \"premise:\", \"sentence 1:\", \"sentence 2:\", \"sentence:\", \"hypothesis:\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVsYfCFNHvAn"
      },
      "outputs": [],
      "source": [
        "def import_dataset(dataset_index, test_set = False):\n",
        "  dataset_name = NLI_DATASETS[dataset_index]\n",
        "  print(dataset_name)\n",
        "  seed = 100 #test_set is equal independently on random seed\n",
        "  if test_set == False:\n",
        "    test_set_path = f\"{dataset_name}/{dataset_name}_16_{seed}_dev.tsv\"\n",
        "  else:\n",
        "    test_set_path = f\"{dataset_name}/{dataset_name}_16_{seed}_test.tsv\"\n",
        "  #print(test_set_path)\n",
        "  df = pd.read_csv(test_set_path , sep=\"\\t\", header=None)\n",
        "\n",
        "  df[0] = df[0].apply(preprocess_nli)\n",
        "  #print(df[0][0])\n",
        "  return (df[0], df[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WmI81aWYH-v"
      },
      "outputs": [],
      "source": [
        "def preprocess_nli(query):\n",
        "  for substr in  WORDS_TO_REMOVE:\n",
        "    query = query.replace(substr, \"\")\n",
        "\n",
        "  ph = re.split(\"\\[SEP\\]\", query)\n",
        "  #return f\"premise:{ph[0]} hypothesis:{ph[1]}\"\n",
        "  return (ph[0], ph[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpFrqbmAHu6k"
      },
      "outputs": [],
      "source": [
        "for dataset_inex in range (0, len(NLI_DATASETS)):\n",
        "  import_dataset(dataset_inex)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hANosoU3giZ1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDKP-1uGIi4P"
      },
      "source": [
        "## PROMPTS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoTg3kbOuuL3"
      },
      "source": [
        "The scitail dataset is setted up for binary NLI: possible answeres are entailment and not entailment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoHi7D5Vhixg"
      },
      "outputs": [],
      "source": [
        "NLI_TRUE_FALSE_PROMPTS = [\n",
        "  \"Premise: {CONTEXT}\\n\\nHypothesis: {HYPOTHESIS}\\n\\nDoes the premise entails the hypothesis?\\n\\n{OPTIONS}\",\n",
        "  \"Premise: {CONTEXT}\\nHypothesis: {HYPOTHESIS}\\nIs the hypothesis entailed by the premise?\\n{OPTIONS}\",\n",
        "  \"Here is a premise:\\n{CONTEXT}\\n\\nHere is a hypothesis:\\n{HYPOTHESIS}\\n\\ nIs it possible to conclude that if the premise is true, then so is the hypothesis?\\n{OPTIONS}\",\n",
        "  \"Sentence 1: {CONTEXT}\\n\\nSentence 2: {HYPOTHESIS}\\nIs this second sentence entailed by the first sentence?\\n\\n{OPTIONS}\",\n",
        "  \"Sentence 1: {CONTEXT}\\n\\nSentence 2: {HYPOTHESIS}\\n\\nIf the first sentence is true, then is the second sentence true?\\n{OPTIONS}\",\n",
        "  'Based on the premise \\\"{CONTEXT}\\\", can we conclude the hypothesis \\\"{HYPOTHESIS}\\\" is true?\\n\\n{OPTIONS}',\n",
        "  'Premise: \\\"{CONTEXT}\\\" If this premise is true, what does that tell us about whether it entails the hypothesis \\\"{HYPOTHESIS}\\\"?\\n\\n{OPTIONS}',\n",
        "  'Premise:\\n\\\"{CONTEXT}\\\" Based on this premise, is the hypothesis \\\"{HYPOTHESIS}\\\" true?\\n {OPTIONS}',\n",
        "  'If {CONTEXT}, can we conclude that \\\"{HYPOTHESIS}\\\"?\\n{OPTIONS}',\n",
        "  '{CONTEXT}\\n\\nDoes it follow that \\\"{HYPOTHESIS}\\\"?\\n{OPTIONS}'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOkwMWUTIoWi"
      },
      "outputs": [],
      "source": [
        "NLI_PROMPTS_SINGLE_SENTENCE = [\n",
        "    \"Choose the correct label among: {OPTIONS} for the following Natural Language Inference task: {SENTENCE}\",\n",
        "    \"Read the follwing sentence. sentence: {SENTENCE}. Is it an 'entailment', a 'contradiction' or is it 'neutral'?\",\n",
        "    \"Assign one of the following labels: {OPTIONS} to the following sentence S: {SENTENCE}\" ,\n",
        "    \"The following assertion: '{SENTENCE}' is {OPTIONS}?\",\n",
        "    \"A sentence can be of one type among: {OPTIONS}. Which type of sentence is [{SENTENCE}]?\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mvBECr3qdCf"
      },
      "outputs": [],
      "source": [
        "NLI_PROMPT_CONTEXT_HYPO = [\n",
        "    \"{CONTEXT}\\nBased on the paragraph above can we conclude that the hypothesis: \\\"{HYPOTHESIS}\\\" is more likely to be a kind of {OPTIONS}? Choose only one of answer among theese three.\",\n",
        "    \"{CONTEXT}\\n\\nBased on that paragraph can we conclude that this sentence is true?\\n{HYPOTHESIS}\\n\\n{OPTIONS}\",\n",
        "    \"{CONTEXT}\\n\\nCan we draw the following conclusion?\\n{HYPOTHESIS}\\n\\n{OPTIONS}\"\n",
        "    \"{CONTEXT}\\nDoes this next sentence follow, given the preceding text?\\n{HYPOTHESIS}\\n\\n{OPTIONS}\",\n",
        "    \"{CONTEXT}\\nCan we infer the following?\\n{HYPOTHESIS}\\n\\n{OPTIONS}\",\n",
        "    \"Read the following paragraph and determine if the hypothesis is true:\\n\\n{CONTEXT}\\n\\nHypothesis: {HYPOTHESIS}\\n\\n{OPTIONS}\",\n",
        "    \"Read the text and determine if the sentence is true:\\n\\n{CONTEXT}\\n\\nSentence: {HYPOTHESIS}\\n\\n{OPTIONS}\",\n",
        "    \"Can we draw the following hypothesis from the CONTEXT? \\n\\nCONTEXT:\\n\\n{CONTEXT}\\n\\nHypothesis: {HYPOTHESIS}\\n\\n{OPTIONS}\",\n",
        "    \"Determine if the sentence is true based on the text below:\\n{HYPOTHESIS}\\n\\n{CONTEXT}\\n{OPTIONS}\",\n",
        "    \"Premise: {CONTEXT}\\n\\nHypothesis: {HYPOTHESIS}\\n\\nDoes the premise entail the hypothesis?\\n\\n{OPTIONS}\",\n",
        "    \"Premise: {CONTEXT}\\nHypothesis: {HYPOTHESIS}\\nIs the hypothesis entailed by the premise?\\n{OPTIONS}\",\n",
        "    \"Here is a premise:\\n{CONTEXT}\\n\\nHere is a hypothesis:\\n{HYPOTHESIS}\\n\\nIs it possible to conclude that if the premise is true, then so is the hypothesis?\\n{OPTIONS}\",\n",
        "    \"Sentence 1: {CONTEXT}\\n\\nSentence 2: {HYPOTHESIS}\\nIs this second sentence entailed by the first sentence?\\n\\n{OPTIONS}\",\n",
        "    \"Sentence 1: {CONTEXT}\\n\\nSentence 2: {HYPOTHESIS}\\n\\nIf the first sentence is true, then is the second sentence true?\\n{OPTIONS}\",\n",
        "    'Based on the premise \"{CONTEXT}\", can we conclude the hypothesis {HYPOTHESIS}\" is true?\\n\\n{OPTIONS}',\n",
        "    'Premise: \"{CONTEXT}\" If this premise is true, what does that tell us about whether it entails the hypothesis \"{HYPOTHESIS}\"?\\n\\n{OPTIONS}',\n",
        "    'Premise:\\n\"{CONTEXT}\" Based on this premise, is the hypothesis \"{HYPOTHESIS}\" true?\\n{OPTIONS}',\n",
        "    'If {CONTEXT}, can we conclude that \"{HYPOTHESIS}\"?\\n{OPTIONS}',\n",
        "    '{CONTEXT}\\n\\nDoes it follow that \"{HYPOTHESIS}\"?\\n{OPTIONS}'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p10HB_Sxk5c6"
      },
      "outputs": [],
      "source": [
        "NLI_PROMPT_CONTEXT_HYPO[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ujbu4Z2tLPBZ"
      },
      "outputs": [],
      "source": [
        "from random import shuffle\n",
        "\n",
        "def adapt_prompt(prompt_index, sentence, context_hypo_type = False, two_options_nli = False):\n",
        "  if two_options_nli == False:\n",
        "    options = ['entailment','neutral', 'contradiction']\n",
        "    shuffle(options)\n",
        "    stringyfied_options = f\"['{options[0]}', '{options[1]}', '{options[2]}']\";\n",
        "\n",
        "    if context_hypo_type:\n",
        "      context, hypo = sentence;\n",
        "      return NLI_PROMPT_CONTEXT_HYPO[prompt_index].format(OPTIONS=stringyfied_options, CONTEXT=context, HYPOTHESIS=hypo)\n",
        "    else:\n",
        "      return NLI_PROMPTS_SINGLE_SENTENCE[prompt_index].format(OPTIONS = options, SENTENCE = sentence)\n",
        "  #if !two_options_nli:\n",
        "  else:\n",
        "    options = ['true','false']\n",
        "    context, hypo = sentence;\n",
        "    return NLI_TRUE_FALSE_PROMPTS[prompt_index].format(OPTIONS=options, CONTEXT=context, HYPOTHESIS=hypo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taSnFAUBZpO7"
      },
      "source": [
        "##MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Xrbgtt68Pwf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIA0skvt8Pwg"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def askLlamaNLI(data, solutions, prompt_index = 0, context_hypo_prompt = False, two_options_nli=False):\n",
        "  results = []\n",
        "  for i in range(len(data)):\n",
        "    task_prefix = adapt_prompt(prompt_index, data[i], context_hypo_prompt, two_options_nli)\n",
        "\n",
        "    comp = solutions.tolist()\n",
        "\n",
        "    candidate_labels = [ \"neutral\", \"entailment\"]  if two_options_nli else [\"contradiction\", \"neutral\", \"entailment\"]\n",
        "\n",
        "    sequence = pipeline(\n",
        "      task_prefix,\n",
        "      candidate_labels = candidate_labels,\n",
        "      max_length=500\n",
        "    )\n",
        "\n",
        "    answer = sequence[\"labels\"][0]\n",
        "\n",
        "    if(answer == solutions[i]):\n",
        "      results.append(True)\n",
        "    else:\n",
        "      results.append(False)\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "A6f5E7L2C_e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_prompts = [\n",
        "    [0,7,8],\n",
        "    [0,2,12],\n",
        "    [0,3,6],\n",
        "    [12,19,21],\n",
        "    [5,6,9]\n",
        "]"
      ],
      "metadata": {
        "id": "yo1xOA34flJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmFxDCnwicNO"
      },
      "outputs": [],
      "source": [
        "str = \"\";\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def test_on_datasets(test_mode = False):\n",
        "  binary_task = False;\n",
        "  for dataset_index in range(3,4): ## len(NLI_DATASETS)):\n",
        "    if dataset_index == 0:\n",
        "      binary_task = True;\n",
        "      single_sentence_propmts_num = 0;\n",
        "      prompt_num = len(NLI_TRUE_FALSE_PROMPTS);\n",
        "    else:\n",
        "      binary_task = False;\n",
        "      single_sentence_propmts_num = len(NLI_PROMPTS_SINGLE_SENTENCE)\n",
        "      prompt_num = single_sentence_propmts_num+len(NLI_PROMPT_CONTEXT_HYPO);\n",
        "\n",
        "    prompt_to_use = range(0, prompt_num) if test_mode == False else best_prompts[dataset_index]\n",
        "    for prompt_index in prompt_to_use:\n",
        "      question, answer = import_dataset(dataset_index, test_mode)\n",
        "\n",
        "      if dataset_index == 2 and test_mode:\n",
        "        question = question[0:1000]\n",
        "        answer = answer[0:1000]\n",
        "\n",
        "      context_hypo_prompt = prompt_index >= single_sentence_propmts_num\n",
        "\n",
        "      prompt_index = prompt_index - single_sentence_propmts_num if context_hypo_prompt else prompt_index;\n",
        "\n",
        "      results = askLlamaNLI(question, answer, prompt_index, context_hypo_prompt, binary_task)\n",
        "\n",
        "      dataset_name = NLI_DATASETS[dataset_index]\n",
        "\n",
        "      corrects = sum(bool(x) for x in results)\n",
        "\n",
        "      str = f\"Model: {model}\\tDataset name: {dataset_name}\\t Prompt with context: {context_hypo_prompt}\\t Prompt index: {prompt_index}\\t - Correct Answers {corrects}/{len(results)} - {corrects/len(results)}\"\n",
        "      print(str)\n",
        "\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## scitail"
      ],
      "metadata": {
        "id": "cIvht31EiuBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_on_datasets()"
      ],
      "metadata": {
        "id": "DuwbrqGEi4Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_on_datasets(True)"
      ],
      "metadata": {
        "id": "oziqHKZNjYSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##anli"
      ],
      "metadata": {
        "id": "VXvg5qRRttQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_on_datasets()"
      ],
      "metadata": {
        "id": "TVZZHdpvttAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_on_datasets(True)"
      ],
      "metadata": {
        "id": "X9x-FL1e2nZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##glue-mnli"
      ],
      "metadata": {
        "id": "hYqQ8Qj4tzAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_on_datasets()"
      ],
      "metadata": {
        "id": "ZohtYHH9t0wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_on_datasets(True)"
      ],
      "metadata": {
        "id": "QZ9swM_M1Yx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##sick"
      ],
      "metadata": {
        "id": "4wcpOE5C0myY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_on_datasets()"
      ],
      "metadata": {
        "id": "ge2_lMgo0ljO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_on_datasets(True)"
      ],
      "metadata": {
        "id": "QYaE8ZiG0okk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## superglue-cb"
      ],
      "metadata": {
        "id": "p0PGA2sBirCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_on_datasets()"
      ],
      "metadata": {
        "id": "XYsTGcsnDxYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_on_datasets(True)"
      ],
      "metadata": {
        "id": "0yMLkmBiev3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example of model behavior with"
      ],
      "metadata": {
        "id": "zNEErlvYdl0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = pipeline(\n",
        "    ' Chose one random letter between [\"A\",\"B\",\"C\"]',\n",
        "      do_sample=True,\n",
        "      top_k=10,\n",
        "      num_return_sequences=1,\n",
        "      #eos_token_id=tokenizer.eos_token_id,\n",
        "      max_length=400\n",
        "    )"
      ],
      "metadata": {
        "id": "6CknvgenapIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence"
      ],
      "metadata": {
        "id": "VP1uFd03Z4I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## time evaluation"
      ],
      "metadata": {
        "id": "9kttSzyDgwGs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DIhZxtO5aMOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvgnpJUfACvf"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnqI0Es6BDeZ"
      },
      "outputs": [],
      "source": [
        "def time_tests(test_mode = True):\n",
        "  binary_task = False;\n",
        "  for dataset_index in range (0, len(NLI_DATASETS)):\n",
        "    if dataset_index == 0:\n",
        "      binary_task = True;\n",
        "      single_sentence_propmts_num = 0;\n",
        "      prompt_num = len(NLI_TRUE_FALSE_PROMPTS);\n",
        "    else:\n",
        "      binary_task = False;\n",
        "      single_sentence_propmts_num = len(NLI_PROMPTS_SINGLE_SENTENCE)\n",
        "      prompt_num = single_sentence_propmts_num+len(NLI_PROMPT_CONTEXT_HYPO);\n",
        "\n",
        "    prompt_index = best_prompts[dataset_index][0]\n",
        "    question, answer = import_dataset(dataset_index, test_mode)\n",
        "\n",
        "    if test_mode:\n",
        "      question = question[0:32]\n",
        "      answer = answer[0:32]\n",
        "\n",
        "    context_hypo_prompt = prompt_index >= single_sentence_propmts_num\n",
        "\n",
        "    prompt_index = prompt_index - single_sentence_propmts_num if context_hypo_prompt else prompt_index;\n",
        "\n",
        "    start = time.time()\n",
        "    results = askLlamaNLI(question, answer, prompt_index, context_hypo_prompt, binary_task)\n",
        "    end = time.time()\n",
        "\n",
        "    dataset_name = NLI_DATASETS[dataset_index]\n",
        "\n",
        "    str = f\"Model: {model}\\tDataset name: {dataset_name}\\t - Time: {end - start}\"\n",
        "    print(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGNpD1EW_rXi"
      },
      "outputs": [],
      "source": [
        "time_tests()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MDKP-1uGIi4P",
        "VXvg5qRRttQs",
        "hYqQ8Qj4tzAy",
        "4wcpOE5C0myY",
        "p0PGA2sBirCu"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
